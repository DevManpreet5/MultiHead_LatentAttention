{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ropeless(nn.Module):\n",
    "    def __init__(self,d_model,kv_latent_dim,num_head):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.kv_latent_dim=kv_latent_dim\n",
    "        self.num_heads=num_head\n",
    "\n",
    "        self.dim_each_head=d_model//num_head\n",
    "\n",
    "        self.W_q=nn.Linear(d_model,d_model,bias=False)\n",
    "        self.W_dkv=nn.Linear(d_model,kv_latent_dim,bias=False)\n",
    "        self.W_uk=nn.Linear(kv_latent_dim,d_model,bias=False)\n",
    "        self.W_uv=nn.Linear(kv_latent_dim,d_model,bias=False)\n",
    "        self.W_o=nn.Linear(d_model,d_model,bias=False)\n",
    "\n",
    "        self.layernom=nn.LayerNorm(kv_latent_dim)\n",
    "        self.register_buffer('absorbed',None)\n",
    "\n",
    "    \n",
    "    def forward(self,x,kv_cache=None,past_length=0):\n",
    "        batch,size1,dim=x.size()\n",
    "\n",
    "        if self.absorbed is None:\n",
    "            absorbed=torch.matmul(self.W_q.weight,self.W_uk.weight)\n",
    "            self.absorbed=absorbed.view(self.num_heads,self.dim_each_head,-1) # basically jitne head hai utne me split krna hoga , if nhead=2 divide absorbed in 2 parts \n",
    "        \n",
    "        new_cache_kv=self.layernom(self.W_dkv(x))\n",
    "\n",
    "        if kv_cache is None:\n",
    "            cache_kv=new_cache_kv\n",
    "        else:\n",
    "            cache_kv=torch.cat([kv_cache,new_cache_kv],dim=1)\n",
    "\n",
    "        \n",
    "        size_full=cache_kv.size(1)\n",
    "        value_full=self.W_uv(cache_kv)\n",
    "        v=value_full.view(batch,size_full,self.num_heads,self.dim_each_head).transpose(1,2)\n",
    "\n",
    "        q=x.view(batch,size1,self.num_heads,self.dim_each_head)\n",
    "        attn_score=torch.zeros(batch,self.num_heads,size1,size_full,device=x.device)\n",
    "\n",
    "        for heads in range(self.num_heads):\n",
    "            scores=torch.matmul(q[:,:,heads],self.absorbed[heads])\n",
    "            attn_score[:,heads]=torch.bmm(scores,cache_kv.transpose(1,2))\n",
    "\n",
    "        attn_score = attn_score / (self.dim_each_head ** 0.5)\n",
    "        mask = torch.tril(torch.ones((size1, size_full), device=x.device), diagonal=past_length)\n",
    "        attn_score = attn_score.masked_fill(mask.view(1, 1, size1, size_full) == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_score, dim=-1)  \n",
    "\n",
    "        out_heads = []\n",
    "        for h in range(self.num_heads):\n",
    "            context_h = torch.matmul(attn_weights[:, h], v[:, h])  \n",
    "            out_heads.append(context_h)\n",
    "\n",
    "        out = torch.cat(out_heads, dim=-1)  \n",
    "        return self.W_o(out), cache_kv\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ropeless' object has no attribute 'dh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, seq_len, d_model)\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m ropeless(d_model\u001b[38;5;241m=\u001b[39md_model, kv_latent_dim\u001b[38;5;241m=\u001b[39mkv_latent_dim, num_head\u001b[38;5;241m=\u001b[39mn_heads)\n\u001b[0;32m---> 13\u001b[0m out, new_cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)        \u001b[38;5;66;03m# Expected: (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_cache\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# Expected: (batch_size, seq_len, kv_latent_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 46\u001b[0m, in \u001b[0;36mropeless.forward\u001b[0;34m(self, x, kv_cache, past_length)\u001b[0m\n\u001b[1;32m     43\u001b[0m     scores\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmatmul(q[:,:,heads],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabsorbed[heads])\n\u001b[1;32m     44\u001b[0m     attn_score[:,heads]\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbmm(scores,cache_kv\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 46\u001b[0m attn_score \u001b[38;5;241m=\u001b[39m attn_score \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdh\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     47\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones((size1, size_full), device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice), diagonal\u001b[38;5;241m=\u001b[39mpast_length)\n\u001b[1;32m     48\u001b[0m attn_score \u001b[38;5;241m=\u001b[39m attn_score\u001b[38;5;241m.\u001b[39mmasked_fill(mask\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, size1, size_full) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ropeless' object has no attribute 'dh'"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "n_heads = 8\n",
    "seq_len = 16\n",
    "batch_size = 2\n",
    "kv_latent_dim = 128 \n",
    "\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "\n",
    "model = ropeless(d_model=d_model, kv_latent_dim=kv_latent_dim, num_head=n_heads)\n",
    "\n",
    "out, new_cache = model(x)\n",
    "\n",
    "print(f\"Output shape: {out.shape}\")        # Expected: (batch_size, seq_len, d_model)\n",
    "print(f\"Cache shape: {new_cache.shape}\")   # Expected: (batch_size, seq_len, kv_latent_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
